# 4 Analysis

```{r, echo = FALSE, message = FALSE}
source(here::here("scripts/setup.R"))
```

**TO DO's**

-   Answers to the research questions
-   Different methods considered
-   Competing approaches
-   Justifications

## 4.2 RQ1

Based on past electric vehicle adoption trends in Switzerland, can we forecast future adoption rates and pinpoint times of significant increases or decreases correlated with major events or policy changes?

### Simple Linear Regression

#### With Oil

Choosing Dependent Variable, sales or EV registrations

Sales Data (from `df_sales_EV`):

Pros: Directly reflects market demand and consumer purchasing behavior.
Cons: Can be influenced by short-term factors such as promotions or
subsidies, which might not indicate long-term adoption trends.

Registration Data (from `df_v_electric`):

Pros: Represents actual additions to the vehicle population and can be
more indicative of long-term trends in EV adoption. Cons: Might lag
behind sales data, as registration occurs post-purchase and can be
influenced by administrative processes.

interested in long-term trends in EV adoption and usage, registration
data could be more appropriate.

The decision to remove rows with missing values was justified as the
number of missing values in the Price column was relatively small (28
out of a larger dataset) and couldn't be reliably imputed, ensuring that
the analysis was performed on a more complete and consistent dataset.

```{r}
# Select the columns you want to keep in df_v_electric
df_v_electric <- df_v_electric %>%
  select(Location, Count, Date)

# Merge df_oil_monthly with df_v_electric based on the common 'Date' column
df_merged <- merge(df_v_electric, df_oil_monthly, by = "Date", all.x = TRUE)

# Remove rows with missing values in the 'Price' column
df_merged <- df_merged[complete.cases(df_merged), ]

# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
split_index <- sample(1:nrow(df_merged), 0.8 * nrow(df_merged))
train_data <- df_merged[split_index, ]
test_data <- df_merged[-split_index, ]

# Fitting the linear model
mod1_lin <- lm(Count ~ Price, data = train_data)

# Summary of the model to view coefficients and statistics
summary(mod1_lin)

# Predicting with the test data
predictions <- predict(mod1_lin, test_data)
# Calculating residuals (difference between actual and predicted values)
residuals <- test_data$Count - predictions

# Calculating RMSE
rmse <- sqrt(mean(residuals^2))
print(paste("RMSE:", rmse))
# Creating a summary table for the linear model
table_regression <- tbl_regression(mod1_lin)

# Viewing the table
table_regression
```

The linear regression analysis results indicate that the Price variable
has a very weak or negligible effect on explaining the variation in the
Count variable. Specifically:

The coefficient for Price is very close to zero (-0.00542), indicating
that there is almost no linear relationship between Price and Count.

The p-value for Price (0.97) is much greater than the common
significance level of 0.05. This high p-value suggests that Price is not
statistically significant in predicting Count, as it fails to reject the
null hypothesis that the coefficient is zero.

The R-squared value is very low (0.000523), indicating that the linear
regression model explains only a negligible fraction of the variance in
Count.

The Root Mean Squared Error (RMSE) is relatively high (247), suggesting
that the model's predictions have a substantial amount of error.

In summary, based on these results, it appears that the Price variable
does not have a meaningful impact on predicting the Count variable, and
the linear regression model is not suitable for explaining the
relationship between these two variables. Further exploration of the
data and potentially considering other factors or modeling approaches
may be necessary to improve predictive accuracy.

#### with google trends

```{r}
# Merge the two datasets based on the common Date column
df_merged <- merge(df_merged, df_gtrends, by = "Date")
# Check for missing values in df_merged
#sum(is.na(df_merged))
# Check for missing values in each column of df_merged
#colSums(is.na(df_merged))

# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
split_index <- sample(1:nrow(df_merged), 0.8 * nrow(df_merged))
train_data <- df_merged[split_index, ]
test_data <- df_merged[-split_index, ]

# Fitting the linear model
mod2_lin <- lm(Count ~ SearchRatio, data = train_data)

# Summary of the model to view coefficients and statistics
summary(mod2_lin)

# Predicting with the test data
predictions <- predict(mod2_lin, test_data)

# Calculating residuals (difference between actual and predicted values)
residuals <- test_data$Count - predictions

# Calculating RMSE
rmse <- sqrt(mean(residuals^2))
print(paste("RMSE:", rmse))

# Creating a summary table for the linear model
table_regression2 <- tbl_regression(mod2_lin)

# Viewing the table
table_regression2
```

Overall, while there is a statistically significant relationship between
SearchRatio and Count, the model's R-squared value suggests that other
factors not included in the model may also influence the Count variable.
You may want to explore additional variables or more complex models to
improve predictive accuracy.

### Multivariable Regression
#### with Demographic groups, Oil Price and Google Trend

```{r}
# Merge the data frames on the 'Year' column
df_merged <- merge(df_merged, df_demographic, by.x = "Date", by.y = "Year")

# Splitting the data into training and testing sets
set.seed(123) # for reproducibility
split_index <- sample(1:nrow(df_merged), 0.8 * nrow(df_merged))
train_data <- df_merged[split_index, ]
test_data <- df_merged[-split_index, ]

# Fitting the multivariable linear model
mod1_multi <- lm(Count ~ SearchRatio + Price + `Generation Z` + Millennials + `Generation X` + `Baby Boomers`, data = df_merged)

# Summary of the model to view coefficients and statistics
summary(mod1_multi)

# Predicting with the test data
predictions <- predict(mod1_multi, test_data)

# Evaluating the model
# Calculating R-squared value
r_squared <- cor(test_data$Count, predictions)^2
cat("R-squared: ", r_squared, "\n")

# Calculating RMSE (Root Mean Squared Error)
rmse <- sqrt(mean((predictions - test_data$Count)^2))
cat("RMSE: ", rmse, "\n")

# Create a summary table for the linear model
table_regression_mod1_multi <- tbl_regression(mod1_multi)

# Print the table
table_regression_mod1_multi
```
This multivariate linear regression model does not appear to have a strong explanatory power, as indicated by the low adjusted R-squared value and the relatively high RMSE. Additionally, only the Baby Boomers variable shows statistical significance in predicting the Count, while other variables do not appear to be significant in this context. You may want to consider further refining the model or exploring other variables to improve its predictive performance.

#### Adding Political Parties
```{r}
# Remove 'Confederation' from df_merged
df_multi_reg <- df_merged[df_merged$Location != 'Confederation', ]

# Step 2: Aggregate data for 'Switzerland' in politic data, For each year, sum the values of the 26 cantons to create a combined observation labeled "Switzerland". and then divide by 26
political_combined_data$Year <- year(ymd(political_combined_data$Year))
df_politics <- political_combined_data %>%
  group_by(Year) %>%
  summarise(across(c(`Against`, `Slightly Against`, `Neutral`, `Slightly in Favour`, `In Favour`), sum, na.rm = TRUE)) %>%
  mutate(Canton = 'Switzerland')

# Append the new 'Switzerland' rows to df_politics
df_politics <- bind_rows(political_combined_data, df_politics)

# List of column names you want to divide by 26
columns_to_divide <- c("Against", "Slightly Against", "Neutral", "Slightly in Favour", "In Favour")

df_politics <- df_politics %>%
  mutate_at(columns_to_divide, list(~./26))

# Step 3: Align 'Date' in df_multi_reg to 'Year' in df_politics 
df_multi_reg$Year <- year(ymd(df_multi_reg$Date))
df_multi_reg <- select(df_multi_reg, -Date)

#Merge the datasets on the aligned 'Location/Canton' and 'Date/Year' columns.
df_politics$Location <- df_politics$Canton 

# Step 4: Merge the datasets on 'Canton' and 'Year'
df_multi_reg <- merge(df_multi_reg, df_politics, by = c("Location", "Year"), all = FALSE)

# Reorder columns in a data frame
df_multi_reg <- df_multi_reg %>%
  select(Count, everything())

df_multi_reg <- df_multi_reg %>% select(-Canton, -KANTONSNUM)
any(is.na(df_multi_reg))

# Perform regression analysis without 'Location'
full_model <- lm(Count ~ Year + Price + SearchRatio + `Generation Z` + Millennials + `Generation X` + `Baby Boomers` + Against + `Slightly Against` + Neutral + `Slightly in Favour` + `In Favour`, data = df_multi_reg)

# Perform backward elimination
reduced_model <- step(full_model, direction = "backward")

# View the summary of the reduced model
summary(reduced_model)

# Perform backward elimination
reduced_model <- step(full_model, direction = "backward")

# View the summary of the reduced model
summary(reduced_model)
# Create a summary table for the linear model
table_regression_mod2_multi <- tbl_regression(reduced_model)

# Print the table
table_regression_mod2_multi
```


1. **Starting Model**: The initial model includes a wide range of predictor variables: Year, Price, SearchRatio, various generational groups (Generation Z, Millennials, Generation X, Baby Boomers), and several categories of opinions (Against, Slightly Against, Neutral, Slightly in Favour, In Favour).

2. **Stepwise Process**: The stepwise procedure iteratively removes the least significant variables based on their contribution to the model (using the Akaike Information Criterion, AIC, as a guide). A lower AIC indicates a better model fit with respect to the number of variables.

3. **Model Refinement**: As we progress through the steps, we observe the removal of several variables like `Baby Boomers`, `Millennials`, and `Generation X`. This suggests that these variables were not significantly contributing to the explanation of variations in `Count`.

4. **Final Model**: The last step shows the model with the variables: SearchRatio, Slightly Against, Neutral, and In Favour. This model has an AIC of 710, which is lower than the starting AIC of 715, indicating a more efficient model.

5. **Significance of Remaining Variables**: In the final model, all the variables are considered significant contributors. If any were to be removed, it would result in a higher AIC, indicating a less optimal model.

6. **Interpretation of Variables**:
   - `SearchRatio`, `Slightly Against`, `Neutral`, and `In Favour` are significant predictors for `Count`.
   - The absence of demographic variables in the final model suggests that the adoption count may not be strongly related to these demographic factors, or their effect is captured by other variables.
   - The presence of opinion-related variables (like `In Favour`) indicates a possible correlation between public opinion and the count of the dependent variable (possibly related to EV adoption or similar context).

7. **Cautions**: While stepwise regression is useful for variable selection, it can sometimes lead to overfitting or neglecting important variables that don't show strong individual effects but are important in combination with others. Hence, the results should be interpreted with caution, and further analysis (like checking for multicollinearity, interactions between variables, etc.) is recommended to validate the findings.

##### Vizualisation of regression Quality

```{r}
# Set up the layout for a 2x2 plot
par(mfrow = c(2, 2))

# Create the diagnostic plots
plot(full_model, 1, main = "Full Model Residuals vs Fitted")
plot(full_model, 2, main = "Full Model Q-Q Plot")
plot(full_model, 3, main = "Full Model Scale-Location Plot")
plot(full_model, 5, main = "Full Model Residuals vs Leverage")

# Reset to default layout
par(mfrow = c(1, 1))
```


In the plot, most of the data points seem to be randomly scattered around the horizontal line at zero, which is good as it suggests that the residuals have constant variance and that the model's predictions are unbiased. However, there are two data points labeled 74 and 76 that stand out from the rest, with one particularly far from zero. These points could be outliers or influential points that have a large effect on the regression line.

The decision to remove outliers should not be taken lightly. Simply removing outliers solely because they are outliers can lead to overfitting the model to the remaining data, and the results may not generalize well to new data. Instead, we should investigate why those data points are outliers:

Are they the result of data entry errors or measurement errors? If so, they should be corrected or removed.

Do they represent a segment of the population that is not well explained by the model? If that's the case, you might need a more complex model.

Are they true extreme values that are part of the natural variation of the data? If they are valid observations, removing them could bias your results.


## 4.3 RQ2

Are there differences in adoption rate within the different regions in Switzerland? And are there different buying behaviors displayed by the demographic segments within Switzerland?

From the initial data:

In 2005, the EV adoption rate per capita was approximately 0.000004,
which means there were about 4 EVs per million people. By 2009, this
rate increased to approximately 18 EVs per million people.

This approach will give us a general sense of EV adoption in relation to
the overall population but won't provide regional demographic
granularity.

The merged data now includes the total population for each year in
Switzerland and the total count of electric vehicles (EVs) for those
years. We have also calculated the EV adoption rate per capita, which
gives us an insight into how EV adoption scales with the population
size.

These figures show a growing trend in EV adoption in relation to the
population size, albeit the numbers are still quite small relative to
the total population.

The trend shows a gradual increase in EV adoption relative to the
population size, indicating a growing acceptance and usage of electric
vehicles in Switzerland during this period.

```{r}
df_demo <- df_demographic
df_ev <- df_v_electric
# Convert Date and Year to Date type
df_ev$Date <- as.Date(df_ev$Date)
df_demo$Year <- as.Date(df_demo$Year)

# Summing up the population for each year
df_demo$total_population <- rowSums(df_demo[,c("Generation Z", "Millennials", "Generation X", "Baby Boomers")])

# Aggregating EV data by year
df_ev_yearly <- df_ev %>%
  group_by(Year = as.Date(format(Date, "%Y-01-01"))) %>%
  summarize(total_ev = sum(Count))

# Merging the datasets
merged_data <- merge(df_ev_yearly, df_demo, by = "Year")

# Calculating EV adoption per capita
merged_data$ev_per_capita <- merged_data$total_ev / merged_data$total_population
```

```{r}
# Creating a ggplot object with your data
p <- ggplot(merged_data, aes(x = Year, y = ev_per_capita, group = 1)) +
  geom_line(color = "darkblue", size = 1) +
  labs(title = "EV Adoption Per Capita Over Time in Switzerland",
       x = "Year",
       y = "EV Adoption Per Capita")

# Animate the plot with gganimate, revealing the line over time
animated_plot <- p +
  transition_reveal(Year)
# Render the animation
animate(animated_plot, renderer = gganimate::gifski_renderer(), width = 600, height = 400, res = 96)


```

The correlation matrix below shows the relationships between the
proportions of different generational groups (Generation Z, Millennials,
Generation X, Baby Boomers) and the EV adoption rate per capita in
Switzerland. The heatmap provides the following insights:

The correlation coefficients indicate the strength and direction of the
relationship between each pair of variables. Positive values suggest a
positive correlation (as one increases, so does the other), while
negative values suggest an inverse relationship.

\*babyboomers !

```{r}
# Data Preparation
df_ev$Date <- as.Date(df_ev$Date)
df_demo$Year <- as.Date(df_demo$Year)

# Summing up the population for each year
df_demo$total_population <- rowSums(df_demo[,c("Generation Z", "Millennials", "Generation X", "Baby Boomers")])

# Calculate proportions
df_demo$prop_gen_z <- df_demo$`Generation Z` / df_demo$total_population
df_demo$prop_millennials <- df_demo$Millennials / df_demo$total_population
df_demo$prop_gen_x <- df_demo$`Generation X` / df_demo$total_population
df_demo$prop_boomers <- df_demo$`Baby Boomers` / df_demo$total_population

# Aggregating EV data by year
df_ev_yearly <- df_ev %>%
  group_by(Year = as.Date(format(Date, "%Y-01-01"))) %>%
  summarize(total_ev = sum(Count))

# Merging the datasets
merged_data <- merge(df_ev_yearly, df_demo, by = "Year")

# Calculating EV adoption per capita
merged_data$ev_per_capita <- merged_data$total_ev / merged_data$total_population

# Correlation Matrix
correlation_matrix <- cor(merged_data[,c("prop_gen_z", "prop_millennials", "prop_gen_x", "prop_boomers", "ev_per_capita")])

# Melting the correlation matrix for ggplot
melted_correlation_matrix <- melt(correlation_matrix)

# Modify the variable names by removing 'prop_' and replacing '_' with ' '
melted_correlation_matrix$Var1 <- gsub("prop_", "", melted_correlation_matrix$Var1)
melted_correlation_matrix$Var2 <- gsub("prop_", "", melted_correlation_matrix$Var2)
melted_correlation_matrix$Var1 <- gsub("_", " ", melted_correlation_matrix$Var1)
melted_correlation_matrix$Var2 <- gsub("_", " ", melted_correlation_matrix$Var2)

# Creating the heatmap
p <- ggplot(melted_correlation_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%.2f", value)), color = "white", size = 4) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "", y = "", title = "Correlation Heatmap")

# Convert to interactive plot
ggplotly(p, tooltip = c("label", "fill"))
```

## 4.3 RQ3
How has the growth of electric vehicles evolved in comparison to other countries such as France, and what factors might account for the differences in their evolution ?


```{r}
# Filtering Swiss data for specific fuel types
swiss_specific_fuel <- df_v %>%
  filter(Fuel %in% c("Diesel", "Electricity", "Conventional hybrid", "Plug-in hybrid", "Petrol")) %>%
  filter(Location == 'Switzerland') |>
  filter(VehicleType == 'Passenger car') |>
  filter(Date > as.Date('2012-01-01')) |>
  filter(Date < as.Date('2021-12-31'))

# Selecting equivalent columns from the French dataset
french_specific_fuel <- df_v_fr %>%
  select(Date, Diesel_delta, Essence_delta, Conventional_Hybrid_delta, Plug_in_Hybrid_delta, Electrique_delta) # Adjust column names accordingly

# Reshape French dataset to long format for easier plotting
french_specific_fuel_long <- french_specific_fuel %>%
  pivot_longer(cols = -Date, names_to = "Fuel", values_to = "Count")

# Standardize counts in each dataset
swiss_specific_fuel <- swiss_specific_fuel %>%
  mutate(Count = scale(Count))

french_specific_fuel_long <- french_specific_fuel_long %>%
  mutate(Count = scale(Count))

# Rename the 'Fuel' column in the French dataset
french_specific_fuel_long <- french_specific_fuel_long %>%
  mutate(Fuel = case_when(
    Fuel == "Diesel_delta" ~ "Diesel",
    Fuel == "Essence_delta" ~ "Petrol",
    Fuel == "Conventional_Hybrid_delta" ~ "Conventional hybrid",
    Fuel == "Plug_in_Hybrid_delta" ~ "Plug-in hybrid",
    Fuel == "Electrique_delta" ~ "Electricity"
  ))

# Define color palette for each fuel type
fuel_colors <- c("Diesel" = "black", "Electricity" = "green", "Conventional hybrid" = "purple", "Plug-in hybrid" = "blue", "Petrol" = "orange")

p <- ggplot() +
  geom_smooth(data = swiss_specific_fuel, aes(x = Date, y = Count, color = Fuel), 
              method = "loess", se = FALSE, size = 1.5) +
  geom_line(data = french_specific_fuel_long, aes(x = Date, y = Count, color = Fuel), 
            alpha = 0.4, size = 0.8) +
  scale_color_manual(values = fuel_colors, 
                     labels = c("Diesel", "Electricity", "Conventional hybrid", 
                                "Plug-in hybrid", "Petrol"),
                     breaks = c("Diesel", "Electricity", "Conventional hybrid", 
                                "Plug-in hybrid", "Petrol")) +
  labs(x = "Date", y = "Standardized Count", color = "Fuel Type") +
  theme_minimal() +
  geom_text(data = data.frame(x = as.Date("2021-01-01"), y = c(3, 2.8), 
                              label = c("Switzerland has", "the thickest line")), 
            aes(x = x, y = y, label = label, color = label), 
            size = 4, show.legend = FALSE)

interactive_plot <- ggplotly(p, width = 600, height = 400, tooltip = c("x", "y", "color"))
interactive_plot <- interactive_plot %>%
  layout(legend = list(orientation = "h", x = 0, xanchor = "left", y = -0.2))
interactive_plot
```
```{r}
# Filtering Swiss data for specific fuel types
swiss_specific_fuel <- df_v %>%
  filter(Fuel %in% c("Electricity", "Conventional hybrid", "Plug-in hybrid")) %>%
  filter(Location == 'Switzerland' & VehicleType == 'Passenger car' & Date > as.Date('2012-01-01') & Date < as.Date('2021-12-31'))

# Selecting equivalent columns from the French dataset
french_specific_fuel <- df_v_fr %>%
  select(Date, Diesel_delta, Essence_delta, Conventional_Hybrid_delta, Plug_in_Hybrid_delta, Electrique_delta) # Adjust column names accordingly

# Reshape French dataset to long format for easier plotting
french_specific_fuel_long <- french_specific_fuel %>%
  pivot_longer(cols = -Date, names_to = "Fuel", values_to = "Count") %>%
  mutate(Fuel = case_when(
    Fuel == "Conventional_Hybrid_delta" ~ "Conventional hybrid",
    Fuel == "Plug_in_Hybrid_delta" ~ "Plug-in hybrid",
    Fuel == "Electrique_delta" ~ "Electricity"
  ))

# Standardize counts in each dataset
swiss_specific_fuel <- swiss_specific_fuel %>%
  mutate(Count = scale(Count))

french_specific_fuel_long <- french_specific_fuel_long %>%
  filter(Fuel %in% c("Conventional hybrid", "Plug-in hybrid", "Electricity")) %>%
  mutate(Count = scale(Count))

# Define color palette for each fuel type
fuel_colors <- c("Conventional hybrid" = "purple", "Plug-in hybrid" = "blue", "Electricity" = "green")

# Plotting with faceting and improved axis text
p <- ggplot() +
  geom_smooth(data = swiss_specific_fuel, aes(x = Date, y = Count, color = Fuel), 
              method = "loess", se = FALSE, size = 1.5) +
  geom_line(data = french_specific_fuel_long, aes(x = Date, y = Count, color = Fuel), 
            alpha = 0.4, size = 0.8) +
  scale_color_manual(values = fuel_colors, labels = c("Switzerland", "France")) +
  labs(x = "Date", y = "Standardized Count") +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        strip.text.x = element_text(size = 10, angle = 0),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.title.x = element_text(size = 12, margin = margin(t = 10)),
        axis.title.y = element_text(size = 12, margin = margin(r = 10))) +
  facet_wrap(~Fuel, scales = 'free_y', ncol = 1)

# Convert to interactive plot
interactive_plot <- ggplotly(p, width = 600, height = 600, tooltip = c("x", "y", "color"))
interactive_plot <- interactive_plot %>%
  layout(legend = list(orientation = "h", x = 0, xanchor = "left", y = -0.2))
interactive_plot
```
Switzerland has the thickest line

This compares the rate at which a certain category of fuel is adopted by the citizen of each country (Switzerland vs. France) and is normalized for each country's size.

We can see that Electricity and Plug-in Hybrid follow roughly the same trajectory. This could be caused by the fact that they are used in a very similar way. In both these categories Switzerland has a slightly higher adoption rate.

For Conventional hybrid however, France seems to have a slightly faster adopting rate. 

```{r}
data <- charge_ch_fr 

# Convert year to Date format and then extract the year
data$year <- as.Date(paste0(data$year, "-01-01"))
data$year <- format(data$year, "%Y")

# Sum the values by year and region
data_summarized <- data %>%
  group_by(year, region) %>%
  summarize(total_value = sum(value))

# Create the ggplot
p <- ggplot(data_summarized, aes(x = year, y = total_value, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("France" = "#4E79A7", "Switzerland" = "#F28E2B")) +  # Adjusted custom colors
  labs(title = "Total Availability of Charging Stations in France vs Switzerland",
       x = "Year",
       y = "Total Charging Stations") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), # Center the plot title
        legend.title = element_blank()) # Remove the legend title

# Convert to interactive plot using plotly
p_interactive <- ggplotly(p)
p_interactive <- ggplotly(p, width = 600, height = 600, tooltip = c("x", "y", "color"))
p_interactive <- p_interactive %>%
  layout(legend = list(orientation = "h", x = 0, xanchor = "left", y = -0.2))
p_interactive
```


## 4.5 RQ4
To what extent does the evolution in the availability of charging stations exert an influence on the adoption of electric vehicles in Switzerland?
```{r}
# First, let's merge the df_v and df_charge_number_CH data sets, and we will look at Fuel: Electricity

df_v_electric_total_ch <- df_v %>%
  filter(Fuel == "Electricity", VehicleType == "Passenger car", Location == c("Switzerland","Confederation")) %>%
  select(Date, Count)

sum_by_year <- df_v_electric_total_ch %>%
  group_by(Year = lubridate::year(Date)) %>%
  summarise(Total_Count = sum(Count))


# Convert year to a common format for merging
sum_by_year <- sum_by_year %>%
  mutate(year = as.Date(paste0(Year, "-01-01")))

# Merge the datasets based on the "year" column
merged_v_charge <- left_join(sum_by_year, df_charge_number_CH, by = c("year" = "year"))

# cleaning merged data set
merged_v_charge <- merged_v_charge %>%
  filter(Year > "2011") %>%
  select(Year, Total_Count, powertrain, value)

names(merged_v_charge)[names(merged_v_charge) == "Total_count"] <- "EVs"
colnames(merged_v_charge)[colnames(merged_v_charge) == "value"] <- "Charging station"

# Summing Powertrain together
merged_v_charge <- merged_v_charge %>%
  group_by(Year, Total_Count) %>%
  summarise(Count = sum(`Charging station`))

# Checking the correlation
corr_charge_ev <- cor(merged_v_charge$Total_Count, merged_v_charge$Count)

# their correlation is 0.957, almost perfectly correlated (no suprise here)

# Checking for lagged correlation

lags_to_explore <- 1:3

lagged_correlation <- function(data, lag) {
  data %>%
    mutate(Count_Lagged = lag(Count, n = lag, default = NA)) %>%
    summarise(Correlation = cor(Total_Count, Count_Lagged, use = "complete.obs"))
}

# Calculate lagged correlations for each lag
lagged_correlations_df <- data.frame(Lag = lags_to_explore) %>%
  rowwise() %>%
  mutate(Correlation = lagged_correlation(merged_v_charge[, -1], Lag)$Correlation)

# Print the results
print("Original Correlation:")
print(corr_charge_ev)

print("Lagged Correlations:")
print(lagged_correlations_df)


# Now we formulate the following Hypothesis

# H0: new charging station increase EV adoption vs. H1: new charging station does not increase EV adoption

# Check these hypotheses with a simple linear regression
linear_charging <- lm(Total_Count ~ Count, data = merged_v_charge)

# Poisson Test
poisson_model <- glm(Total_Count ~ Count, family = poisson, data = merged_v_charge)

# Set up the layout using mfrow
par(mfrow = c(1, 2))  # 1 row, 2 columns

# Plotting for Simple Linear Regression
plot(linear_charging, 1, main = "LM Residuals vs Fitted")
plot(linear_charging, 2, main = "LM Normal Q-Q Plot")
plot(linear_charging, 3, main = "LM Scale-Location Plot")
plot(linear_charging, 5, main = "LM Residuals vs Leverage")

# Plotting for Poisson Regression
plot(poisson_model, which = 1, main = "Poisson Residuals vs Fitted")
plot(poisson_model, which = 2, main = "Poisson Normal Q-Q Plot")
plot(poisson_model, which = 3, main = "Poisson Scale-Location Plot")
plot(poisson_model, which = 5, main = "Poisson Residuals vs Leverage")

# Printing
summary(linear_charging)
summary(poisson_model)
# Reset the layout
par(mfrow = c(1, 1))
```
The year-on-year correlation is the highest, the lagged correlation diminishes. We could suggest hat the availability of charging stations and the new registration of EVs go hand-in-hand, and that the availability of new charging station does not create a demand of new EVs by itself. Correlation does not imply causation, while we see a relationship, we can't conclude that charging stations directly cause changes in electric vehicle adoption only with a Correlation analysis

With both a linear regression and a Poisson-test. We find evidence of statistically significant relationship between the count of available charging station and the count of electric vehicles registered. We have Prediction variable/coefficient of 2.68 x 10^-4 and 3.34 respectively. And a p-val < 0.005 for both. However, it is important to remind ourselves that these variables have a bidirectional / mutual influence, beyond the scope of what our analysis shows. The relationship is not strictly unidirectional and therefore, it is hard to conclude anything without further domain-knowledge and context-specific information

